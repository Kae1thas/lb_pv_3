# Лабораторная работа №3: Параллельная реализация метода сопряжённых градиентов для решения СЛАУ

## Цель работы
Интегрировать знания и навыки, полученные в предыдущих работах, для реализации полного параллельного алгоритма решения системы линейных алгебраических уравнений (СЛАУ) методом сопряжённых градиентов с использованием библиотеки `mpi4py`. Исследовать эффективность и масштабируемость реализации.

### Стек технологий
- **Язык программирования**: Python
- **Библиотеки**: `mpi4py`, `numpy`, `matplotlib` (для визуализации)
- **Реализация MPI**: OpenMPI или MPICH

## Теоретическая часть
Метод сопряжённых градиентов является итерационным алгоритмом для решения СЛАУ (Ax = b), особенно эффективным для больших, разреженных или плохо обусловленных систем. Параллельная реализация требует:
1. Декомпозиции данных: Разделения матрицы A (по строкам) и векторов b, x, r, p, q между процессами.
2. Координации вычислений: Использования коллективных операций MPI (Allreduce, Allgatherv, Reduce_scatter) для синхронизации промежуточных результатов (скалярных произведений, сложения векторов) между процессами.
3. Оптимизации обменов: Минимизации объёма передаваемых данных и количества коммуникационных операций для достижения хорошего ускорения.

## Этапы реализации

### Этап 1: Упрощённая версия
- Реализована в файле `cg_simple.py`.
- Выполняет параллельное решение СЛАУ методом сопряжённых градиентов с базовым распределением данных.
- Использует операции `MPI.Bcast`, `MPI.Scatterv`, `MPI.Allreduce` для координации.
- Результаты времени выполнения выводятся процессом 0.

### Этап 2: Полная параллельная версия
- Реализована в файле `parallel_cg.py`.
- Расширяет упрощённую версию, включая:
  - Декомпозицию вектора x по процессам.
  - Использование `MPI.Allgatherv` для сборки полного вектора x.
  - `MPI.Reduce_scatter` для распределения невязки r.
  - Сбор итогового решения с помощью `MPI.Gatherv`.
- Результаты времени выполнения и ошибки относительно `numpy.linalg.lstsq` выводятся процессом 0.

### Этап 3: Генерация тестовых данных
- Реализована в файле `generate.py`.
- Создаёт файлы `in.dat`, `AData.dat`, `bData.dat`, `xTrue.dat` с заданными размерами (N = 500, M = 1000).
- Использует `numpy.random` для генерации матрицы A и вектора b с контролируемой обусловленностью.

### Этап 4: Анализ производительности
- Реализован в файле `speedup.py`.
- Генерирует графики ускорения (speedup) и эффективности (efficiency) для упрощённой и полной версий при 2, 4 и 8 процессах.
- Результаты сохранены в файл `cg_speedup_efficiency.png`.

## Тестовые данные
- **Файлы**:
  - `in.dat`: Содержит размеры N и M (500 1000).
  - `AData.dat`: Матрица A размером M × N.
  - `bData.dat`: Вектор b длиной M.
  - `xTrue.dat`: Истинное решение длиной N (для верификации).

## График ускорения и эффективности
График (cg_speedup_efficiency.png) показывает:
- **Speedup**: Упрощённая версия достигает ~2.1 при 8 процессах, полная версия ~3.2.
- **Efficiency**: Упрощённая версия падает до ~0.4 при 8 процессах, полная версия до ~0.5.
- Анализ: Полная версия демонстрирует лучшее масштабирование благодаря оптимизации коммуникаций, но эффективность снижается из-за накладных расходов MPI при увеличении числа процессов.
<img width="1000" height="500" alt="cg_speedup_efficiency" src="https://github.com/user-attachments/assets/599e98da-a92d-4908-8b54-da627ffdd981" />

