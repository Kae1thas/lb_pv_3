# Лабораторная работа №3: Параллельная реализация метода сопряжённых градиентов для решения СЛАУ

## Цель работы
Интегрировать знания и навыки, полученные в предыдущих работах, для реализации полного параллельного алгоритма решения системы линейных алгебраических уравнений (СЛАУ) методом сопряжённых градиентов с использованием библиотеки `mpi4py`. Исследовать эффективность и масштабируемость реализации.

### Стек технологий
- **Язык программирования**: Python
- **Библиотеки**: `mpi4py`, `numpy`, `matplotlib` (для визуализации)
- **Реализация MPI**: OpenMPI

## Тестовые данные
- **Файлы**:
  - `in.dat`: Содержит размеры N и M (500 1000).
  - `AData.dat`: Матрица A размером M × N.
  - `bData.dat`: Вектор b длиной M.
  - `xTrue.dat`: Истинное решение длиной N (для верификации).

## Этапы реализации

### Этап 0: Генерация тестовых данных
- Реализована в файле `generate.py`.
- Создаёт файлы `in.dat`, `AData.dat`, `bData.dat`, `xTrue.dat` с заданными размерами (N = 500, M = 1000).
- Использует `numpy.random` для генерации матрицы A и вектора b с контролируемой обусловленностью.

### Этап 1: Упрощённая версия
- Реализована в файле `cg_simple.py`.
- Выполняет параллельное решение СЛАУ методом сопряжённых градиентов с базовым распределением данных.
- Использует операции `MPI.Bcast`, `MPI.Scatterv`, `MPI.Allreduce` для координации.
- Результаты времени выполнения выводятся процессом 0.

<img width="1100" height="256" alt="image" src="https://github.com/user-attachments/assets/0aacfd1d-109d-4529-9e03-ebe3ff00e678" />

### Этап 2: Полная параллельная версия
- Реализована в файле `parallel_cg.py`.
- Расширяет упрощённую версию, включая:
  - Декомпозицию вектора x по процессам.
  - Использование `MPI.Allgatherv` для сборки полного вектора x.
  - `MPI.Reduce_scatter` для распределения невязки r.
  - Сбор итогового решения с помощью `MPI.Gatherv`.
- Результаты времени выполнения и ошибки относительно `numpy.linalg.lstsq` выводятся процессом 0.

<img width="1129" height="222" alt="image" src="https://github.com/user-attachments/assets/b77613e1-cc1e-4dbc-b078-509e436d0a71" />

### Этап 3: Анализ производительности
- Реализован в файле `speedup.py`.
- Генерирует графики ускорения (speedup) и эффективности (efficiency) для упрощённой и полной версий при 2, 4 и 8 процессах.
- Результаты сохранены в файл `cg_speedup_efficiency.png`.

## График ускорения и эффективности
График (cg_speedup_efficiency.png) показывает:
- **Speedup**: Упрощённая версия достигает ~2.1 при 8 процессах, полная версия ~3.2.
- **Efficiency**: Упрощённая версия падает до ~0.4 при 8 процессах, полная версия до ~0.5.
- Анализ: Полная версия демонстрирует лучшее масштабирование благодаря оптимизации коммуникаций, но эффективность снижается из-за накладных расходов MPI при увеличении числа процессов.
<img width="1000" height="500" alt="cg_speedup_efficiency" src="https://github.com/user-attachments/assets/2ed417ed-6483-40c2-b144-68d8998d561b" />

## Анализ результатов
### Описание реализованных алгоритмов
- **Упрощённая версия (`cg_simple.py`)**: Распределяет матрицу A и вектор b между всеми процессами, включая процесс 0, и выполняет итерации метода сопряжённых градиентов с использованием базовых операций MPI. Вектор x инициализируется и обновляется на всех процессах, а итоговое решение собирается только на процессе 0.
- **Полная версия (`parallel_cg.py`)**: Улучшает упрощённую версию, добавляя декомпозицию вектора x и использование более сложных операций MPI (Allgatherv, Reduce_scatter, Gatherv). Это позволяет распределить вычисления более равномерно и снизить объём передаваемых данных на некоторых этапах.

### Объяснение наблюдаемого ускорения
Ускорение наблюдается благодаря параллельной обработке данных на нескольких процессах. Полная версия показывает лучшее ускорение (~3.2 при 8 процессах) по сравнению с упрощённой (~2.1), так как более эффективно использует декомпозицию вектора x и оптимизирует коммуникации. Однако рост ускорения замедляется с увеличением числа процессов из-за накладных расходов на синхронизацию и обмен данными.

### Сравнение двух версий
- **Преимущества полной версии**: Лучшее масштабирование (максимальный speedup выше), более равномерное распределение вычислений.
- **Недостатки полной версии**: Большие накладные расходы на коммуникации (Allgatherv, Reduce_scatter), что снижает эффективность при большом числе процессов.
- **Преимущества упрощённой версии**: Меньше коммуникационных операций, что делает её менее чувствительной к накладным расходам при малом числе процессов.
- **Недостатки упрощённой версии**: Ограниченная масштабируемость из-за централизованного управления вектором x.

### Узкие места (bottlenecks)
- **Коммуникационные задержки**: Операции MPI, такие как Allreduce и Allgatherv, становятся узким местом при увеличении числа процессов из-за роста объёма передаваемых данных и времени синхронизации.
- **Небаланс нагрузки**: При неравномерном распределении строк матрицы A между процессами (особенно при малых M/N) некоторые процессы могут простаивать.
- **Объём данных**: Чтение и передача большой матрицы A (500 × 1000) из файла замедляет начальную фазу работы, особенно для процесса 0.

## Выводы
Метод сопряжённых градиентов в параллельной реализации с использованием MPI демонстрирует хорошую эффективность для решения больших СЛАУ (например, M = 1000, N = 500) при умеренном числе процессов (2-4). Полная версия лучше подходит для задач с большим числом переменных (N), где декомпозиция x даёт значительное преимущество. Однако при увеличении числа процессов свыше 8 эффективность падает из-за роста накладных расходов на коммуникации. Для задач с ещё большим M и N рекомендуется оптимизировать распределение данных и минимизировать использование дорогостоящих операций MPI. Метод применим для разреженных систем и задач, где вычисления доминируют над коммуникациями, но требует тщательной настройки для достижения оптимальной производительности.
